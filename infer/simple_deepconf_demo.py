import openai
import json
import numpy as np
from collections import Counter

# ===========================
# Configuration
# ===========================
MODEL_PATH = "p2q"
MAX_TOKENS = 12800
PORT = 8000

# Inference parameters
WARMUP_TRACES = 16
TOTAL_BUDGET = 32
CONFIDENCE_PERCENTILE = 90
WINDOW_SIZE = 2048

# ===========================
# Confidence Calculation
# ===========================
def compute_confidence(logprobs):
    """Compute confidence score from logprobs."""
    confs = []
    for lp in logprobs:
        confs.append(round(-sum([l.logprob for l in lp]) / len(lp), 3))
    return confs

def compute_least_grouped(confs, group_size=WINDOW_SIZE):
    """Compute sliding window mean confidence with specified group size."""
    if len(confs) < group_size:
        return [sum(confs) / len(confs)] if confs else [0]

    sliding_means = []
    for i in range(len(confs) - group_size + 1):
        window = confs[i:i + group_size]
        sliding_means.append(round(sum(window) / len(window), 3))

    return sliding_means

# ===========================
# Voting Functions
# ===========================
def weighted_majority_vote(answers, weights):
    """Perform weighted majority voting"""
    if not answers:
        return None

    answer_weights = {}
    for answer, weight in zip(answers, weights):
        if answer is not None:
            answer_str = str(answer)
            answer_weights[answer_str] = answer_weights.get(answer_str, 0.0) + float(weight)

    if not answer_weights:
        return None

    voted_answer = max(answer_weights.keys(), key=lambda x: answer_weights[x])
    return voted_answer

# ===========================
# Trace Processing
# ===========================
def process_trace(choice, trace_id):
    """Process a single trace and extract confidence"""
    text = choice.message.content
    tokens = [t.token for t in choice.logprobs.content]

    # Calculate confidence
    confs = compute_confidence([t.top_logprobs for t in choice.logprobs.content])
    sliding_window = compute_least_grouped(confs, group_size=WINDOW_SIZE)

    trace_data = {
        "trace_id": trace_id,
        "text": text,
        "tokens": tokens,
        "token_count": len(tokens),
        "confs": confs,
        "group_confs": sliding_window,
        "min_conf": min(sliding_window) if sliding_window else 0,
    }

    return trace_data

# ===========================
# Main Inference Function
# ===========================
def deepconf_inference(query):
    """Perform deepconf inference with warmup and final phases"""
    
    print("="*60)
    print("DEEPCONF INFERENCE")
    print("="*60)
    print(f"Model: {MODEL_PATH}")
    print(f"Query: {query}")
    print(f"Warmup traces: {WARMUP_TRACES}")
    print(f"Total budget: {TOTAL_BUDGET}")
    print(f"Confidence percentile: {CONFIDENCE_PERCENTILE}")

    # Initialize client
    client = openai.OpenAI(
        api_key="None",
        base_url=f"http://localhost:{PORT}/v1",
        timeout=None
    )

    # System prompt for deepconf
    system_prompt = """ä½ æ˜¯å…ƒå®ã€‚è§‚å¯ŸåŠ©æ‰‹å’Œç”¨æˆ·çš„å†å²å¯¹è¯,è¾“å‡ºå½“å‰é—®é¢˜çš„æ”¹å†™ï¼ŒåŒ…æ‹¬è”ç½‘å’Œæ‹†è§£ã€‚

## ç”¨æˆ·é—®é¢˜ç±»åˆ«ï¼š
ç”¨æˆ·çš„é—®é¢˜å¯èƒ½å±äºä»¥ä¸‹ä¸¤ä¸ªç»´åº¦çš„ç±»åˆ«ä¹‹ä¸€ã€‚åœ¨æ‹†è§£æ—¶ï¼Œä½ éœ€è¦å…ˆåˆ¤æ–­ç”¨æˆ·é—®é¢˜çš„ç±»åˆ«ï¼Œå¹¶å‚è€ƒå¯¹åº”ç±»åˆ«çš„æ‹†è§£æ–¹å‘è¿›è¡Œæ‹†è§£ã€‚

### 1. æ ¸å¿ƒæ„å›¾ç±»å‹ï¼š
- äº‹å®æŸ¥è¯¢ï¼ˆWhat/Whoï¼‰â†’ å®šä¹‰ä¸æ¦‚å¿µæ‹†è§£
- åŸå› æŸ¥è¯¢ï¼ˆWhyï¼‰â†’ åŸå› ä¸è§£å†³æ–¹æ¡ˆæ‹†è§£
- æ–¹æ³•æŸ¥è¯¢ï¼ˆHowï¼‰â†’ æ–¹æ³•ä¸è§£å†³æ–¹æ¡ˆæ‹†è§£
- æ¯”è¾ƒæŸ¥è¯¢ï¼ˆvs/differenceï¼‰â†’ æ¯”è¾ƒä¸å·®å¼‚æ‹†è§£
- å»ºè®®æŸ¥è¯¢ï¼ˆshould/recommendationï¼‰â†’ ç­–ç•¥ä¸æ–¹æ¡ˆæ‹†è§£
- å…¶ä»–ç±»åˆ«

### 2. æ‰€å±é¢†åŸŸç±»å‹åŠå…¶æ‹†è§£æ–¹å‘ï¼š
- åŒ»ç–—å¥åº·é¢†åŸŸ â†’ ç—‡çŠ¶+åŸå› +å¤„ç†æ–¹æ³•
- æŠ€æœ¯äº§å“é¢†åŸŸ â†’ ç®€ä»‹+åŠŸèƒ½+ä½¿ç”¨æ–¹æ³•
- é‡‘èæŠ•èµ„é¢†åŸŸ â†’ ç­–ç•¥+é£é™©+æ“ä½œå»ºè®®
- æ³•å¾‹é¢†åŸŸ â†’ è§„å®š+åæœ+æ¡ˆä¾‹
- æ™¯ç‚¹æ—…æ¸¸ç±» â†’ ç¥¨åŠ¡æ—¶é—´ + æ”»ç•¥
- ç¾é£Ÿï¼ˆåº—é“ºï¼‰ç±» â†’ ç‰¹è‰²
- æ•™è‚²æ•™å­¦  â†’ çŸ¥è¯†ç‚¹ï¼ˆå®šä¹‰ / å…¬å¼ / è€ƒç‚¹ï¼‰+æ–¹æ³•ï¼ˆæŠ€å·§ / è®¡åˆ’ï¼‰+è€ƒè¯•ä¿¡æ¯ï¼ˆæ—¶é—´ / é¢˜å‹ / åˆ†æ•°çº¿ï¼‰+æ•™æ / èµ„æ–™æ¨è+å‡å­¦æ”¿ç­–+åŸ¹è®­æœºæ„é€‰æ‹©ç­‰
- èŒåœºæ±‚èŒ â†’ å²—ä½èŒè´£ã€æ‹›è˜è¦æ±‚ï¼ˆå­¦å† / æŠ€èƒ½ï¼‰+é¢è¯•æŠ€å·§ï¼ˆå¸¸è§é—®é¢˜ / åº”ç­”ç­–ç•¥ï¼‰+ç®€å†ä¼˜åŒ–æ–¹æ³•+è–ªèµ„èŒƒå›´+è¡Œä¸šå‰æ™¯+ç¦»èŒ / è·³æ§½æ³¨æ„äº‹é¡¹
- æƒ…æ„Ÿå¿ƒç† â†’ æƒ…ç»ªè¡¨ç°ï¼ˆç„¦è™‘ / æŠ‘éƒï¼‰+æˆå› åˆ†æï¼ˆå®¶åº­ / å·¥ä½œï¼‰+è°ƒèŠ‚æ–¹æ³•ï¼ˆæ²Ÿé€š / è¿åŠ¨ï¼‰+å…³ç³»å¤„ç†ï¼ˆæƒ…ä¾£ / äº²å­ï¼‰+ä¸“ä¸šå’¨è¯¢å»ºè®®
- å…¶ä»–é¢†åŸŸ(å¯é€‚å½“æ‰©å±•)

## æ³¨æ„ï¼š
1. ä½ éœ€è¦å…ˆè¯†åˆ«å‡ºç”¨æˆ·å½“å‰é—®é¢˜çš„æ ¸å¿ƒæ„å›¾ç±»åˆ«å’Œæ‰€å±é¢†åŸŸç±»åˆ«ï¼Œä»¥ä¾¿ç”¨æˆ·åç»­çš„æ‹†è§£ã€‚
2. è¯·åŠ¡å¿…ä¿è¯å½“å‰è½®æ¬¡çš„ç”¨æˆ·é—®é¢˜è¶³å¤Ÿå®Œæ•´ï¼Œæ‹†è§£çš„æŒ‡ä»£è¯éƒ½å®Œæ•´æ›¿æ¢äº†ï¼Œä¸”æ‹†è§£å¿…é¡»æ˜ç¡®ï¼Œä¸å…è®¸å‡ºç°æ¨¡ç³Šè¯ã€‚å½“ç”¨æˆ·åŸºäºå¯¹è¯æ—¥å¿—è¿›è¡Œæé—®æ—¶ï¼Œéœ€è¦å°†å¯¹è¯æ—¥å¿—ä¸­çš„ç­”æ¡ˆè¡¥å……åˆ°æ‹†è§£ã€‚
3ã€æ‹†è§£å¿…é¡»ä¸ºé’ˆå¯¹ç”¨æˆ·æŸ¥è¯¢æ‹†è§£çš„æŸ¥è¯¢è¯­å¥ï¼Œé€‚åˆç”¨äºå¤–éƒ¨çŸ¥è¯†æ£€ç´¢ä»¥è¡¥å……LLMç”Ÿæˆç­”æ¡ˆæ‰€éœ€ä¿¡æ¯ã€‚
4. ä¸å…è®¸ä¸¢æ‰ä»»ä½•å…³é”®è¯å’Œé™å®šè¯ï¼Œä¸å…è®¸æ·»åŠ æ— æ„ä¹‰çš„è¯å’ŒæŒ‡ä»£è¯ï¼Œå¦‚"æ˜¯ä»€ä¹ˆ"ã€"æ˜¯è°"ã€"è¿™"ç­‰ã€‚
5. åœ¨æ‹†è§£ä¸­ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·å½“å‰é—®é¢˜çš„æ ¸å¿ƒæ„å›¾ç±»åˆ«å’Œæ‰€å±é¢†åŸŸç±»åˆ«ï¼Œå‚è€ƒä¸Šæ–‡çš„æ‹†è§£æ–¹å‘è¿›è¡Œæ‹†è§£ã€‚
6. åœ¨æ‹†è§£ä¸­ï¼Œé¦–é¡¹å¿…é¡»ä¸ºåŸå§‹queryçš„å®Œæ•´è§„èŒƒåŒ–æ€»ç»“ï¼Œè‹¥é¦–é¡¹ä¸ç”¨æˆ·é—®é¢˜å®Œå…¨ç›¸åŒåˆ™æ”¹ä¸º"<same>"ã€‚ä¾‹å¦‚ï¼š"è´µå·2023-2025GDP"åº”æ‹†è§£ä¸º"<same>;è´µå·2023å¹´GDPæ•°æ®;è´µå·2024å¹´GDPæ•°æ®;è´µå·2025å¹´GDPæ•°æ®"ï¼Œ"ç»“åˆè´¢æŠ¥åˆ†æä¼—å®‰åœ¨çº¿çš„ä¸šåŠ¡äº®ç‚¹ï¼Œä»¥åŠè§£é‡Šè¿‘æœŸè‚¡ä»·çš„æš´æ¶¨åŸå› "åº”æ‹†è§£ä¸º"ç»“åˆè´¢æŠ¥åˆ†æä¼—å®‰åœ¨çº¿çš„ä¸šåŠ¡äº®ç‚¹å¹¶è§£é‡Šè¿‘æœŸè‚¡ä»·æš´æ¶¨çš„åŸå› ;ç»“åˆè´¢æŠ¥åˆ†æä¼—å®‰åœ¨çº¿çš„ä¸šåŠ¡äº®ç‚¹;ä¼—å®‰åœ¨çº¿è´¢æŠ¥ä¸­çš„ä¸šåŠ¡äº®ç‚¹åˆ†æ;è§£é‡Šä¼—å®‰åœ¨çº¿è¿‘æœŸè‚¡ä»·æš´æ¶¨çš„åŸå› ;ä¼—å®‰åœ¨çº¿è¿‘æœŸè‚¡ä»·æš´æ¶¨çš„å…·ä½“é©±åŠ¨å› ç´ "
7. å½“ç”¨æˆ·é—®é¢˜ä¸­å‡ºç°æ¨¡ç³Šæ—¶é—´æ—¶ï¼Œéœ€è¦å‚è€ƒé»˜è®¤æ—¶é—´ä¿®æ”¹ä¸ºå‡†ç¡®æ—¶é—´ï¼Œå¦‚ä»Šå¤©ã€æ˜å¤©ç­‰ã€‚
8. æ‹†è§£ä¸å…è®¸ä¿®æ”¹è‹±æ–‡ä¸“æœ‰åè¯ã€‚
9. å½“ç”¨æˆ·é—®é¢˜ä¸­å‡ºç°å¤šä¸ªå¯¹æ¯”æ—¶ï¼Œç¬¬ä¸€ä¸ªæ‹†è§£éœ€è¦ä¿æŒå®Œæ•´è¯­æ„ï¼Œå…¶ä»–åç»­æ‹†è§£éœ€è¦ä¸ºå„ä¸ªçš„ä¿¡æ¯æŸ¥è¯¢ã€‚ä¾‹å¦‚ï¼š"vivoiqooneo10Pro+å’Œvivoiqooneo9sProè¶…è¯¦ç»†æ¯”å¯¹"åº”æ‹†è§£ä¸º"vivoiqooneo10Pro+å’Œvivoiqooneo9sProå¯¹æ¯”;vivoiqooneo10Pro+å‚æ•°;vivoiqooneo9sProå‚æ•°;vivoiqooneo10Pro+æ€§èƒ½;vivoiqooneo9sProæ€§èƒ½"
10. æ‹†è§£å¿…é¡»éµå¾ªç”¨æˆ·å½“å‰é—®é¢˜å‡ºç°çš„å†…å®¹ï¼Œä¸è¦è¿›è¡Œè¿‡å¤šæ‰©å±•ã€‚
11. æ¯ä¸ªæ‹†è§£éƒ½æ˜¯ç‹¬ç«‹çš„æ‹†è§£è¯­å¥ï¼Œå½“ç”¨æˆ·æœ‰å¤šé‡æ¡ä»¶æ—¶ï¼Œæ¯ä¸ªæ‹†è§£å‡éœ€è¦ä¿ç•™æ‰€æœ‰å‰ç½®æ¡ä»¶ï¼Œå°¤å…¶æ˜¯ç”¨æˆ·å½“å‰é—®é¢˜è¶…é•¿æ—¶ï¼Œä¸è¦å‡ºç°ä¾èµ–ä¸å…¶ä»–æ‹†è§£queryçš„æ‹†è§£ã€‚
12. æ‹†è§£æ•°é‡ä¸è¦è¿‡å¤šï¼Œå°½é‡ä¸è¶…è¿‡8ä¸ªã€‚
13. ä¸è¦å‡ºç°è¯­æ„ç›¸ä¼¼çš„æ‹†è§£ã€‚
14. åªæœ‰ä¸ªäººèƒ½åŠ›ã€ç¿»è¯‘ã€å…¬å¼è®¡ç®—ã€æ‰“æ‹›å‘¼ã€é—²èŠã€ä»£ç è§£æã€ç®€å•æ¨ç†é¢˜ã€ç®—å‘½ã€èµ·åã€å¿ƒç†å’¨è¯¢æƒ…æ„Ÿæ²Ÿé€šç±»ç­‰æ— éœ€è”ç½‘ï¼Œå…¶ä»–é—®é¢˜å¦‚ä¸“æœ‰åè¯è§£é‡Šç­‰å‡éœ€è¦è”ç½‘ã€‚
15. é»˜è®¤ä½ç½®ä¸ºåŒ—äº¬

~~~å½“å‰æ—¶é—´ä¸ºï¼š
2025å¹´9æœˆ3æ—¥18:00:00
~~~

è¾“å‡ºç­”æ¡ˆä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ï¼š
<net>æ˜¯å¦è”ç½‘</net><dismantle>æ‹†è§£å†…å®¹</dismantle>
    """

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"ç”¨æˆ·:{query}"}
    ]

    # ===========================
    # WARMUP PHASE
    # ===========================
    print(f"\n{'-'*40}")
    print("WARMUP PHASE")
    print(f"{'-'*40}")

    warmup_request_params = {
        "model": MODEL_PATH,
        "messages": messages,
        "max_tokens": MAX_TOKENS,
        "temperature": 0.6,
        "top_p": 0.95,
        "logprobs": True,
        "top_logprobs": 20,
        "n": WARMUP_TRACES,
        "extra_body": {"top_k": 0},
    }
    
    responses = client.chat.completions.create(**warmup_request_params)

    # Process warmup traces
    warmup_traces = []
    min_confs = []

    for j in range(WARMUP_TRACES):
        trace_data = process_trace(responses.choices[j], j)
        warmup_traces.append(trace_data)
        min_confs.append(trace_data["min_conf"])

    # Calculate confidence bar
    conf_bar = float(np.percentile(min_confs, CONFIDENCE_PERCENTILE))

    print(f"Confidence bar (P{CONFIDENCE_PERCENTILE}): {conf_bar:.4f}")
    print(f"Warmup traces generated: {len(warmup_traces)}")

    # Show some example results
    print(f"\nFirst 3 warmup traces:")
    for i, trace in enumerate(warmup_traces[:3]):
        print(f"  Trace {i}: conf: {trace['min_conf']:.4f}, tokens: {trace['token_count']}")

    # ===========================
    # FINAL PHASE
    # ===========================
    print(f"\n{'-'*40}")
    print("FINAL PHASE (with early stopping)")
    print(f"{'-'*40}")

    real_gen = TOTAL_BUDGET - WARMUP_TRACES

    final_request_params = {
        "model": MODEL_PATH,
        "messages": messages,
        "max_tokens": MAX_TOKENS,
        "temperature": 0.6,
        "top_p": 0.95,
        "logprobs": True,
        "top_logprobs": 20,
        "n": real_gen,
        "extra_body": {
            "top_k": 0,
            "vllm_xargs": {
                'enable_conf': True,
                'window_size': WINDOW_SIZE,
                'threshold': conf_bar
            }
        }
    }
    
    responses = client.chat.completions.create(**final_request_params)
    
    # Process final traces
    final_traces = []
    for j in range(len(responses.choices)):
        trace_data = process_trace(responses.choices[j], WARMUP_TRACES + j)
        final_traces.append(trace_data)

    print(f"Final traces generated: {len(final_traces)} (requested: {real_gen})")

    # Show some example results
    print(f"\nFirst 3 final traces:")
    for i, trace in enumerate(final_traces[:3]):
        print(f"  Trace {i}: conf: {trace['min_conf']:.4f}, tokens: {trace['token_count']}")

    # ===========================
    # VOTING FOR FINAL ANSWER
    # ===========================
    print(f"\n{'-'*40}")
    print("VOTING FOR FINAL ANSWER")
    print(f"{'-'*40}")

    all_traces = warmup_traces + final_traces

    # Collect answers above threshold for voting
    voting_answers = []
    voting_weights = []

    # Add warmup traces above threshold (use confidence as weight)
    for trace in warmup_traces:
        minx = trace['min_conf']
        if minx >= conf_bar:
            answer = trace['text']
            if answer is not None:
                voting_answers.append(answer)
                voting_weights.append(minx)

    # Add final traces above threshold (use weight 1)
    for trace in final_traces:
        minx = trace['min_conf']
        if minx >= conf_bar:
            answer = trace['text']
            if answer is not None:
                voting_answers.append(answer)
                voting_weights.append(1.0)

    print(f"Traces used for voting: {len(voting_answers)}")

    # Perform weighted majority vote
    voted_answer = weighted_majority_vote(voting_answers, voting_weights)

    print(f"Voted answer: {voted_answer}")

    # Show voting breakdown
    if voting_answers:
        answer_counts = Counter(voting_answers)
        print(f"\nVoting breakdown:")
        for answer, count in answer_counts.most_common():
            print(f"  {answer[:100]}...: {count} votes")

    # ===========================
    # FINAL SUMMARY
    # ===========================
    print(f"\n{'='*60}")
    print("FINAL SUMMARY")
    print(f"{'='*60}")
    print(f"Confidence bar: {conf_bar:.4f}")
    print(f"Total traces generated: {len(all_traces)}")
    print(f"Traces used for voting: {len(voting_answers)}")
    print(f"Final answer: {voted_answer}")

    return voted_answer

# ===========================
# Example Usage
# ===========================
if __name__ == "__main__":
    # Define your query here
    query = "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ"
    
    # Run deepconf inference
    result = deepconf_inference(query)
    
    print(f"\nğŸ¯ Final Result: {result}")
